{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzY_jAqw__vw",
        "outputId": "2757d919-1744-4ab8-ac56-2aa0e631b896"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/pctl_idual\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKBoZAsQAxB7",
        "outputId": "8acf0131-4f68-486c-911e-424d50348000"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/pctl_idual\n",
            "dp_solvers.py  idual_solvers.py  lp_solvers.py\t  __pycache__\n",
            "gridworld.py   __init__.py\t pctl_solvers.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs('/root/mosek', exist_ok=True)\n",
        "\n",
        "!cp \"/content/drive/MyDrive/mosek/mosek.lic\" \"/root/mosek/mosek.lic\"\n",
        "\n",
        "print(\"Copied license to /root/mosek/mosek.lic\")\n",
        "\n",
        "!pip install mosek"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOpHu33WURxL",
        "outputId": "cc3986fd-dad9-4183-fa37-8aeb311356e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copied license to /root/mosek/mosek.lic\n",
            "Collecting mosek\n",
            "  Downloading mosek-11.1.3-cp39-abi3-manylinux2014_x86_64.whl.metadata (697 bytes)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from mosek) (2.0.2)\n",
            "Downloading mosek-11.1.3-cp39-abi3-manylinux2014_x86_64.whl (15.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cvxpy as cp\n",
        "\n",
        "x = cp.Variable()\n",
        "y = cp.Variable()\n",
        "\n",
        "prob = cp.Problem(\n",
        "    cp.Minimize(x + y),\n",
        "    [\n",
        "        x + 2*y >= 1,\n",
        "        x >= 0,\n",
        "        y >= 0,\n",
        "    ]\n",
        ")\n",
        "\n",
        "prob.solve(solver=cp.MOSEK)\n",
        "\n",
        "print(\"status:\", prob.status)\n",
        "print(\"x,y =\", x.value, y.value)\n",
        "print(\"objective:\", prob.value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20m0d-0LTDLn",
        "outputId": "2fd24878-a73a-4ac0-929b-d466480136a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "status: optimal\n",
            "x,y = 0.0 0.5\n",
            "objective: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(cp.installed_solvers())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbdgTfLWTkZt",
        "outputId": "95cb2066-03f9-40ab-e82d-c33f3bf1153d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['CLARABEL', 'CVXOPT', 'GLPK', 'GLPK_MI', 'HIGHS', 'MOSEK', 'OSQP', 'SCIPY', 'SCS']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Global LP"
      ],
      "metadata": {
        "id": "fzsGNeTXz1ip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, os\n",
        "import time\n",
        "#parent of pctl_idual\n",
        "sys.path.append(\"/content/drive/MyDrive\")\n",
        "\n",
        "# print(\"Top-level MyDrive:\", os.listdir(\"/content/drive/MyDrive\"))\n",
        "# print(\"Inside pctl_idual:\", os.listdir(\"/content/drive/MyDrive/pctl_idual\"))\n",
        "\n",
        "import importlib\n",
        "# Reload in case you edited the files\n",
        "import pctl_idual.gridworld as gw\n",
        "import pctl_idual.dp_solvers as dp\n",
        "import pctl_idual.lp_solvers as lp\n",
        "import pctl_idual.pctl_solvers as ps\n",
        "\n",
        "importlib.reload(gw)\n",
        "importlib.reload(dp)\n",
        "importlib.reload(lp)\n",
        "importlib.reload(ps)\n",
        "\n",
        "from pctl_idual.gridworld import (\n",
        "    make_grid_world,\n",
        "    print_cost_grid,\n",
        "    make_4x4_world,\n",
        "    make_4x4_pctl_world,\n",
        "    make_20x20_world,\n",
        "    make_NxN_world\n",
        ")\n",
        "\n",
        "from pctl_idual.dp_solvers import (\n",
        "    value_iteration_shortest_path,\n",
        "    greedy_policy_from_V,\n",
        "    simulate_policy,\n",
        ")\n",
        "from pctl_idual.lp_solvers import (\n",
        "    solve_shortest_path_lp,\n",
        "    recover_policy_from_x,\n",
        "    print_policy_grid\n",
        ")\n",
        "\n",
        "from pctl_idual.pctl_solvers import (\n",
        "    RegionFlagSpec,\n",
        "    PCTLRegionConstraint,\n",
        "    UntilSpec,\n",
        "    UntilConstraint,\n",
        "    AugmentedMDP,\n",
        "    solve_lp_with_pctl_aug,\n",
        "    recover_policy_from_x_aug,\n",
        "    print_policy_grid_z0,\n",
        "    print_policy_for_flags,\n",
        "    simulate_policy_aug\n",
        ")\n",
        "\n",
        "def collapse_augmented_policy_to_base(mdp_aug, policy_aug):\n",
        "    \"\"\"\n",
        "    For each base state s, combine action probabilities\n",
        "    across *all* flag valuations into one base policy.\n",
        "    (simple average over reachable augmented states)\n",
        "    \"\"\"\n",
        "    base_policy = {s: {} for s in mdp_aug.base.states}\n",
        "\n",
        "    for st_aug, act_probs in policy_aug.items():\n",
        "        s = st_aug[0]  # physical state\n",
        "        for a, p in act_probs.items():\n",
        "            base_policy[s][a] = base_policy[s].get(a, 0.0) + p\n",
        "\n",
        "    # normalize each state's probs\n",
        "    for s, ap in base_policy.items():\n",
        "        total = sum(ap.values())\n",
        "        if total > 0:\n",
        "            for a in ap:\n",
        "                ap[a] /= total\n",
        "\n",
        "    return base_policy\n",
        "\n",
        "\n",
        "# ------- choose which world to use -------\n",
        "\n",
        "WORLD = \"experiment\"      # options: \"4x4\", \"20x20\", \"custom\"\n",
        "USE_PCTL = (WORLD == \"experiment\")\n",
        "\n",
        "if WORLD == \"4x4_pctl\":\n",
        "    # Base MDP\n",
        "    mdp = make_4x4_pctl_world()\n",
        "\n",
        "    # Define A and B regions for A U B\n",
        "\n",
        "    G2 = {(r, c) for r in range(2, 3) for c in range(0, 2)}  # row 2, cols 0–1\n",
        "    G3 = {(r, c) for r in range(1, 2) for c in range(0, 2)}  # row 1, cols 0–1\n",
        "    # G2 = {(0, c) for c in range(4)} | {(r, 0) for r in range(4)}\n",
        "    # G4 = {(r, c) for r in range(2, 4) for c in range(1, 4)}\n",
        "    # G3 = {(r, c) for r in range(1, 2) for c in range(1, 2)}  #\n",
        "\n",
        "    flags = [\n",
        "        RegionFlagSpec(\"G2\", G2),\n",
        "        RegionFlagSpec(\"G4\", G4),\n",
        "        RegionFlagSpec(\"G3\", G3),\n",
        "    ]\n",
        "    until_specs = [\n",
        "        UntilSpec(\"G2U_G3\", A_region=G2, B_region=G3),\n",
        "    ]\n",
        "\n",
        "    mdp_aug = AugmentedMDP(mdp, flags=flags, until_specs=until_specs)\n",
        "\n",
        "    p_goal_min = 1  # force eventually reaching the main goal with prob 1\n",
        "\n",
        "    region_constraints = [\n",
        "        #P(ever visit G2) ≤ 0   (hard avoidance)\n",
        "        # PCTLRegionConstraint(\"visit_region_min\", \"G3\", 1)\n",
        "        # PCTLRegionConstraint(\"visit_region_max\", \"G4\", 0),\n",
        "        # PCTLRegionConstraint(\"visit_region_max\", \"G4\", 1),\n",
        "    ]\n",
        "\n",
        "    until_constraints = [\n",
        "        # P >= 0.7 [ G2 U G3 ]\n",
        "        # UntilConstraint(\"until_min\", \"G2U_G3\", 0.7),\n",
        "        # UntilConstraint(\"until_min\", \"G2U_G3\", 1)\n",
        "    ]\n",
        "\n",
        "    J_pctl, p_goal, x_opt_aug, region_probs, until_probs, t_pctl = solve_lp_with_pctl_aug(\n",
        "        mdp_aug,\n",
        "        p_goal_min=p_goal_min,\n",
        "        region_constraints=region_constraints,\n",
        "        until_constraints=until_constraints,\n",
        "    )\n",
        "\n",
        "    if x_opt_aug is None:\n",
        "        print(\"\\n[Global PCTL+Until LP] No feasible policy.\")\n",
        "    else:\n",
        "        print(\"\\n=== Global LP with PCTL + Until ===\")\n",
        "        print(\"Optimal expected cost:\", float(J_pctl))\n",
        "        print(\"P(reach GOAL):\", float(p_goal))\n",
        "        for name, val in region_probs.items():\n",
        "            print(f\"P(ever visit {name}):\", float(val))\n",
        "        for name, val in until_probs.items():\n",
        "            print(f\"P({name}):\", float(val))\n",
        "        print(f\"Time taken to solve dual LP:\", round(t_pctl, 3), \"s\")\n",
        "\n",
        "        policy_aug = recover_policy_from_x_aug(mdp_aug, x_opt_aug)\n",
        "        base_traj_pctl, aug_traj_pctl = simulate_policy_aug(mdp_aug, policy_aug)\n",
        "        print(\"\\nTrajectory under PCTL-constrained policy (base states):\", base_traj_pctl)\n",
        "\n",
        "        # total_bits = len(flags) + 2 * len(until_specs)\n",
        "\n",
        "        # print(\"\\nPolicy at flags = all zero (start semantics)\")\n",
        "        # print_policy_for_flags(\n",
        "        #     mdp_aug,\n",
        "        #     policy_aug,\n",
        "        #     flag_tuple=(0,) * total_bits\n",
        "        # )\n",
        "\n",
        "        # Example: “inside G2, ongoing A U B”\n",
        "        # Here flags layout is: [G2][G2U_G3_success][G2U_G3_fail]\n",
        "        # print(\"\\nPolicy when already in G2, not yet hit B:\")\n",
        "        # print_policy_for_flags(mdp_aug, policy_aug, flag_tuple=(1, 0, 0))\n",
        "\n",
        "        # print(\"\\nPolicy after success of G2 U G3:\")\n",
        "        # print_policy_for_flags(mdp_aug, policy_aug, flag_tuple=(1, 1, 0))\n",
        "\n",
        "        base_policy = collapse_augmented_policy_to_base(mdp_aug, policy_aug)\n",
        "\n",
        "        print(\"\\nFinal policy (collapsed to base MDP):\")\n",
        "        print_policy_grid(mdp, base_policy)\n",
        "\n",
        "elif WORLD == \"20x20_pctl\":\n",
        "\n",
        "    mdp = make_20x20_world()\n",
        "    goal = {(0, 19)}\n",
        "    N = 20\n",
        "\n",
        "    G_corridor = set()\n",
        "\n",
        "    # bottom horizontal segment: (19,0) to (19,5)\n",
        "    for c in range(0, 6):\n",
        "        G_corridor.add((19, c))\n",
        "\n",
        "    # vertical segment: (18,5) up to (11,5)\n",
        "    for r in range(18, 10, -1):   # 18,17,...,11\n",
        "        G_corridor.add((r, 5))\n",
        "\n",
        "    G2 = G_corridor\n",
        "    G3 = {(11, 5)}\n",
        "\n",
        "\n",
        "    # G2 = {(r, c) for r in range(8, 12) for c in range(16, 20)}\n",
        "    # G3 = {(r, c) for r in range(0, 8)  for c in range(0, 8)}\n",
        "\n",
        "    flags = [\n",
        "        RegionFlagSpec(\"G2\", G2),\n",
        "        RegionFlagSpec(\"G3\", G3),\n",
        "\n",
        "    ]\n",
        "    until_specs = [\n",
        "        UntilSpec(\"G2U_G3\", A_region=G2, B_region=G3),\n",
        "\n",
        "    ]\n",
        "\n",
        "    mdp_aug = AugmentedMDP(mdp, flags=flags, until_specs=until_specs)\n",
        "    print(f\"Total augmented states (|S_aug|): {len(mdp_aug.states_aug)}\")\n",
        "\n",
        "\n",
        "    p_goal_min = 1 # force eventually reaching the main goal with prob 1\n",
        "\n",
        "    region_constraints = [\n",
        "        # P(ever visit G3) ≤ 0   (hard avoidance)\n",
        "        # PCTLRegionConstraint(\"visit_region_min\", \"G2\", 1),\n",
        "        # PCTLRegionConstraint(\"visit_region_min\", \"G3\", 0.5),\n",
        "    ]\n",
        "\n",
        "    until_constraints = [\n",
        "        # P >= 0.7 [ G2 U G3 ]\n",
        "        UntilConstraint(\"until_min\", \"G2U_G3\", 1),\n",
        "    ]\n",
        "\n",
        "    J_pctl, p_goal, x_opt_aug, region_probs, until_probs, t_pctl = solve_lp_with_pctl_aug(\n",
        "        mdp_aug,\n",
        "        p_goal_min=p_goal_min,\n",
        "        region_constraints=region_constraints,\n",
        "        until_constraints=until_constraints,\n",
        "    )\n",
        "\n",
        "    if x_opt_aug is None:\n",
        "        print(\"\\n[Global PCTL+Until LP] No feasible policy.\")\n",
        "    else:\n",
        "        print(\"\\n=== Global LP with PCTL + Until ===\")\n",
        "        print(\"Optimal expected cost:\", float(J_pctl))\n",
        "        print(\"P(reach GOAL):\", float(p_goal))\n",
        "        for name, val in region_probs.items():\n",
        "            print(f\"P(ever visit {name}):\", float(val))\n",
        "        for name, val in until_probs.items():\n",
        "            print(f\"P({name}):\", float(val))\n",
        "        print(f\"Time taken to solve dual LP:\", round(t_pctl, 3), \"s\")\n",
        "\n",
        "        policy_aug = recover_policy_from_x_aug(mdp_aug, x_opt_aug)\n",
        "        base_traj_pctl, aug_traj_pctl = simulate_policy_aug(mdp_aug, policy_aug)\n",
        "        print(\"\\nTrajectory under PCTL-constrained policy (base states):\", base_traj_pctl)\n",
        "\n",
        "        base_policy = collapse_augmented_policy_to_base(mdp_aug, policy_aug)\n",
        "\n",
        "        print(\"\\nFinal policy (collapsed to base MDP):\")\n",
        "        print_policy_grid(mdp, base_policy, G2=G2, G3=G3)\n",
        "\n",
        "\n",
        "\n",
        "elif WORLD == \"4x4\":\n",
        "    mdp = make_4x4_world()\n",
        "\n",
        "elif WORLD == \"20x20\":\n",
        "    mdp = make_20x20_world()\n",
        "\n",
        "elif WORLD == \"custom\":\n",
        "    # Example custom config:\n",
        "    N = 50\n",
        "    start = (49, 0)\n",
        "    goal = {(0, 49)}\n",
        "\n",
        "    rect_costs = [\n",
        "        (16, 16, 19, 19, 0.1),  # cheap block\n",
        "        (5, 5, 14, 14, 10.0),   # expensive block\n",
        "    ]\n",
        "\n",
        "    mdp = make_grid_world(\n",
        "        N=N,\n",
        "        start=start,\n",
        "        goal=goal,\n",
        "        default_cost=1.0,\n",
        "        rect_costs=rect_costs,\n",
        "        slip_prob=0.0\n",
        "    )\n",
        "\n",
        "\n",
        "elif WORLD == \"experiment\":\n",
        "    Ns = [5, 20, 30, 40, 50, 70, 100, 150, 200, 250]\n",
        "    # Ns = [5, 7, 10, 15, 20, 25]\n",
        "    # Example custom config:\n",
        "    for N in Ns:\n",
        "      print(\"\\n==============================\")\n",
        "      print(f\"Experiment for N = {N}\")\n",
        "      print(\"==============================\")\n",
        "      mdp = make_NxN_world(N)\n",
        "\n",
        "      goal = {(0, N-1)}\n",
        "      G2 = {\n",
        "        (r,c)\n",
        "        for r in range(N)\n",
        "        for c in range(N)\n",
        "        if (r, c) not in goal\n",
        "      }\n",
        "      G3 = goal\n",
        "      flags = [\n",
        "        RegionFlagSpec(\"G2\", G2),\n",
        "        RegionFlagSpec(\"G3\", G3),\n",
        "\n",
        "      ]\n",
        "      until_specs = [\n",
        "        UntilSpec(\"G2U_G3\", A_region=G2, B_region=G3),\n",
        "\n",
        "      ]\n",
        "\n",
        "      mdp_aug = AugmentedMDP(mdp, flags=flags, until_specs=until_specs)\n",
        "      # basic size info\n",
        "      S_aug = len(mdp_aug.states_aug)\n",
        "      non_abs = sum(\n",
        "            1 for st in mdp_aug.states_aug\n",
        "            if not mdp_aug.is_absorbing_aug(st)\n",
        "      )\n",
        "      print(f\"|S_base| = {N*N}\")\n",
        "      print(f\"|S_aug|  = {S_aug}\")\n",
        "      print(f\"non-absorbing aug states: {non_abs}\")\n",
        "\n",
        "      p_goal_min = 1\n",
        "\n",
        "      region_constraints = [\n",
        "\n",
        "      ]\n",
        "\n",
        "      until_constraints = [\n",
        "        # P >= 1 [ G2 U G3 ]\n",
        "        UntilConstraint(\"until_min\", \"G2U_G3\", 1),\n",
        "      ]\n",
        "      t0 = time.perf_counter()\n",
        "      J_pctl, p_goal, x_opt_aug, region_probs, until_probs, t_pctl = solve_lp_with_pctl_aug(\n",
        "      mdp_aug,\n",
        "      p_goal_min=p_goal_min,\n",
        "      region_constraints=region_constraints,\n",
        "      until_constraints=until_constraints,\n",
        "      )\n",
        "      t1 = time.perf_counter()\n",
        "      time_global = t1 - t0\n",
        "       # 6) Count the number of x-variables\n",
        "      # If x_opt_aug is a dict: {(st,a): value}\n",
        "      if isinstance(x_opt_aug, dict):\n",
        "          num_x = len(x_opt_aug)\n",
        "      else:\n",
        "          # if it's a numpy array or cvxpy vector:\n",
        "          num_x = x_opt_aug.size\n",
        "\n",
        "      print(f\"global LP x-vars (edges):   {num_x}\")\n",
        "      print(f\"optimization time:      {t_pctl:.4f} s\")\n",
        "      print(f\"full pipeline time:  {time_global:.4f} s\")\n",
        "      print(f\"P(reach GOAL):             {p_goal}\")\n",
        "      print(f\"P(G2U_G3):                 {until_probs.get('G2U_G3')}\")\n",
        "      print(f\"Objective value:                 {J_pctl:.4f}\")\n",
        "\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"Unknown WORLD='{WORLD}'\")\n",
        "\n",
        "if not USE_PCTL:\n",
        "  # DP ground truth\n",
        "  V = value_iteration_shortest_path(mdp)\n",
        "  print(\"DP (unconstrained) optimal cost from START:\", V[mdp.start])\n",
        "\n",
        "  pi = greedy_policy_from_V(mdp, V)\n",
        "  traj = simulate_policy(mdp, pi)\n",
        "  print(\"Trajectory under DP (unconstrained):\", traj)\n",
        "  # LP solution\n",
        "  J_lp, x_opt, t_lp = solve_shortest_path_lp(mdp)\n",
        "  print(\"LP cost:\", J_lp, \"   solve time:\", t_lp)\n",
        "\n",
        "  pi_lp = recover_policy_from_x(mdp, x_opt)\n",
        "  print(\"\\nPolicy from LP:\")\n",
        "  print_policy_grid(mdp, pi_lp)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLWVjsRhBJ76",
        "outputId": "9ecef4ca-daf2-4fea-fd24-2ba0c6fa7aa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "Experiment for N = 5\n",
            "==============================\n",
            "|S_base| = 25\n",
            "|S_aug|  = 400\n",
            "non-absorbing aug states: 384\n",
            "=== Global LP size (PCTL+until) ===\n",
            "  non-absorbing aug states : 384\n",
            "  edges / x-vars           : 1536\n",
            "  constraints              : 3\n",
            "global LP x-vars (edges):   1536\n",
            "optimization time:      0.0260 s\n",
            "full pipeline time:  0.0424 s\n",
            "P(reach GOAL):             1.0\n",
            "P(G2U_G3):                 1.0\n",
            "Objective value:                 1.7000\n",
            "\n",
            "==============================\n",
            "Experiment for N = 20\n",
            "==============================\n",
            "|S_base| = 400\n",
            "|S_aug|  = 6400\n",
            "non-absorbing aug states: 6384\n",
            "=== Global LP size (PCTL+until) ===\n",
            "  non-absorbing aug states : 6384\n",
            "  edges / x-vars           : 25536\n",
            "  constraints              : 3\n",
            "global LP x-vars (edges):   25536\n",
            "optimization time:      3.8098 s\n",
            "full pipeline time:  4.1035 s\n",
            "P(reach GOAL):             1.0000000002324219\n",
            "P(G2U_G3):                 1.0000000000881024\n",
            "Objective value:                 31.7000\n",
            "\n",
            "==============================\n",
            "Experiment for N = 30\n",
            "==============================\n",
            "|S_base| = 900\n",
            "|S_aug|  = 14400\n",
            "non-absorbing aug states: 14384\n",
            "=== Global LP size (PCTL+until) ===\n",
            "  non-absorbing aug states : 14384\n",
            "  edges / x-vars           : 57536\n",
            "  constraints              : 3\n",
            "global LP x-vars (edges):   57536\n",
            "optimization time:      22.5449 s\n",
            "full pipeline time:  23.2908 s\n",
            "P(reach GOAL):             1.0000000000193843\n",
            "P(G2U_G3):                 1.0000000000054448\n",
            "Objective value:                 51.7000\n",
            "\n",
            "==============================\n",
            "Experiment for N = 40\n",
            "==============================\n",
            "|S_base| = 1600\n",
            "|S_aug|  = 25600\n",
            "non-absorbing aug states: 25584\n",
            "=== Global LP size (PCTL+until) ===\n",
            "  non-absorbing aug states : 25584\n",
            "  edges / x-vars           : 102336\n",
            "  constraints              : 3\n",
            "global LP x-vars (edges):   102336\n",
            "optimization time:      91.3523 s\n",
            "full pipeline time:  92.7105 s\n",
            "P(reach GOAL):             1.00000000003416\n",
            "P(G2U_G3):                 1.0000000000080962\n",
            "Objective value:                 71.7000\n",
            "\n",
            "==============================\n",
            "Experiment for N = 50\n",
            "==============================\n",
            "|S_base| = 2500\n",
            "|S_aug|  = 40000\n",
            "non-absorbing aug states: 39984\n",
            "=== Global LP size (PCTL+until) ===\n",
            "  non-absorbing aug states : 39984\n",
            "  edges / x-vars           : 159936\n",
            "  constraints              : 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I-dual"
      ],
      "metadata": {
        "id": "Hc3MN1NeRfg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, os\n",
        "import time\n",
        "#parent of pctl_idual\n",
        "sys.path.append(\"/content/drive/MyDrive\")\n",
        "\n",
        "# print(\"Top-level MyDrive:\", os.listdir(\"/content/drive/MyDrive\"))\n",
        "# print(\"Inside pctl_idual:\", os.listdir(\"/content/drive/MyDrive/pctl_idual\"))\n",
        "\n",
        "import importlib\n",
        "# Reload in case you edited the files\n",
        "import pctl_idual.gridworld as gw\n",
        "import pctl_idual.dp_solvers as dp\n",
        "import pctl_idual.lp_solvers as lp\n",
        "import pctl_idual.pctl_solvers as ps\n",
        "\n",
        "importlib.reload(gw)\n",
        "importlib.reload(dp)\n",
        "importlib.reload(lp)\n",
        "importlib.reload(ps)\n",
        "\n",
        "from pctl_idual.gridworld import (\n",
        "    make_grid_world,\n",
        "    print_cost_grid,\n",
        "    make_4x4_world,\n",
        "    make_4x4_pctl_world,\n",
        "    make_20x20_world,\n",
        "    make_NxN_world\n",
        ")\n",
        "\n",
        "from pctl_idual.dp_solvers import (\n",
        "    value_iteration_shortest_path,\n",
        "    greedy_policy_from_V,\n",
        "    simulate_policy,\n",
        ")\n",
        "from pctl_idual.lp_solvers import (\n",
        "    solve_shortest_path_lp,\n",
        "    recover_policy_from_x,\n",
        "    print_policy_grid\n",
        ")\n",
        "\n",
        "from pctl_idual.pctl_solvers import (\n",
        "    RegionFlagSpec,\n",
        "    PCTLRegionConstraint,\n",
        "    UntilSpec,\n",
        "    UntilConstraint,\n",
        "    AugmentedMDP,\n",
        "    solve_lp_with_pctl_aug,\n",
        "    recover_policy_from_x_aug,\n",
        "    print_policy_grid_z0,\n",
        "    print_policy_for_flags,\n",
        "    simulate_policy_aug\n",
        ")\n",
        "import importlib\n",
        "\n",
        "# ---- i-dual solvers ----\n",
        "import pctl_idual.idual_solvers as ids\n",
        "importlib.reload(ids)\n",
        "\n",
        "from pctl_idual.idual_solvers import (\n",
        "    compute_in_flow_aug,\n",
        "    solve_lp3_aug,\n",
        "    i_dual_aug,\n",
        "    i_dual_aug_trevizan,\n",
        "    compute_max_prob_visit_region_before_goal,\n",
        "    compute_min_prob_visit_region_before_goal,\n",
        "    compute_max_prob_until_before_goal,\n",
        "    compute_min_prob_until_before_goal,\n",
        ")\n",
        "\n",
        "WORLD = \"experiment\"\n",
        "\n",
        "if WORLD == \"4x4_pctl_idual\":\n",
        "    # 1) Base MDP\n",
        "    mdp = make_4x4_pctl_world()\n",
        "\n",
        "    # 2) Define regions\n",
        "    G2 = {(r, c) for r in range(2, 3) for c in range(0, 2)}  # row 2, cols 0–1\n",
        "    G3 = {(r, c) for r in range(1, 2) for c in range(0, 2)}  # row 1, cols 0–1\n",
        "    # G2 = {(0, c) for c in range(4)} | {(r, 0) for r in range(4)}\n",
        "    # G4 = {(r, c) for r in range(2, 4) for c in range(1, 4)}\n",
        "    # G3 = {(r, c) for r in range(1, 2) for c in range(1, 2)}  #\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # 3) Flags and until specs\n",
        "    flags = [\n",
        "        RegionFlagSpec(\"G2\", G2),\n",
        "        RegionFlagSpec(\"G4\", G4),\n",
        "        RegionFlagSpec(\"G3\", G3),\n",
        "\n",
        "    ]\n",
        "\n",
        "    until_specs = [\n",
        "        UntilSpec(\"G2U_G3\", A_region=G2, B_region=G3),\n",
        "    ]\n",
        "\n",
        "    mdp_aug = AugmentedMDP(mdp, flags=flags, until_specs=until_specs)\n",
        "\n",
        "    region_constraints = [\n",
        "        # P(ever visit G4) ≤ 0   (hard avoidance of G4)\n",
        "        # PCTLRegionConstraint(\"visit_region_max\", \"G4\", 0.5),\n",
        "        # PCTLRegionConstraint(\"visit_region_min\", \"G3\", 0.5),\n",
        "\n",
        "    ]\n",
        "\n",
        "    until_constraints = [\n",
        "        # P >= 0.7 [ G2 U G3 ]\n",
        "        UntilConstraint(\"until_min\", \"G2U_G3\", 1),\n",
        "        # UntilConstraint(\"until_min\", \"G2U_G3\", 1.0)\n",
        "    ]\n",
        "    constraints_region = region_constraints\n",
        "    constraints_full = region_constraints + until_constraints  # for global LP\n",
        "\n",
        "    p_goal_min = 1\n",
        "    # 1) Unconstrained DP on the base MDP\n",
        "    V_base = value_iteration_shortest_path(mdp)  # returns optimal cost-to-go from each base state\n",
        "\n",
        "    H_cost = {st_aug: V_base[st_aug[0]] for st_aug in mdp_aug.states_aug}\n",
        "\n",
        "    H_region_upper = {\n",
        "    # \"G4\": compute_min_prob_visit_region_before_goal(mdp_aug, \"G4\")\n",
        "    }\n",
        "\n",
        "    H_region_lower = {\n",
        "    \"G3\": compute_max_prob_visit_region_before_goal(mdp_aug, \"G3\")\n",
        "    }\n",
        "\n",
        "    H_until_lower = {\n",
        "    # used for until_min (P >= bound)\n",
        "    \"G2U_G3\": compute_max_prob_until_before_goal(mdp_aug, \"G2U_G3\")\n",
        "    }\n",
        "    H_until_upper = {\n",
        "    # used for until_min (P >= bound)\n",
        "    \"G2U_G3\": compute_min_prob_until_before_goal(mdp_aug, \"G2U_G3\")\n",
        "    }\n",
        "\n",
        "    H_max = compute_max_prob_until_before_goal(mdp_aug, \"G2U_G3\")\n",
        "    p_star = H_max[mdp_aug.start_aug]\n",
        "    print(p_star)\n",
        "\n",
        "\n",
        "    # 2) Build heuristic on augmented states\n",
        "    H = {}\n",
        "    for st_aug in mdp_aug.states_aug:\n",
        "        s = st_aug[0]  # physical state\n",
        "        H[st_aug] = V_base.get(s, 0.0)\n",
        "\n",
        "    (\n",
        "        x_final,\n",
        "        goal_prob_final,\n",
        "        region_flag_final,\n",
        "        until_final,\n",
        "        total_time,\n",
        "        final_lp_time,\n",
        "        n_iters,\n",
        "        envelope_size,\n",
        "        obj_final\n",
        "    ) = i_dual_aug_trevizan(\n",
        "        mdp_aug,\n",
        "        p_goal_min=p_goal_min,\n",
        "        extra_constraints=constraints_full,\n",
        "        H=H_cost,\n",
        "        H_region_upper=H_region_upper,\n",
        "        H_region_lower = H_region_lower,\n",
        "        H_until_upper=H_until_upper,\n",
        "        H_until_lower=H_until_lower,\n",
        "    )\n",
        "\n",
        "    if x_final is None:\n",
        "      print(\"\\n[i-dual] No feasible policy under these constraints.\")\n",
        "    else:\n",
        "      from collections import defaultdict\n",
        "      flow_per_base = defaultdict(float)\n",
        "      for (st, a), val in x_final.items():\n",
        "        if val is None:\n",
        "            continue\n",
        "        (r, c) = st[0]\n",
        "        flow_per_base[(r, c)] += float(val)\n",
        "\n",
        "\n",
        "\n",
        "    print(\"\\n=== timing summary ===\")\n",
        "    print(\"total i-dual time:\", total_time)\n",
        "    print(\"final LP time:\", final_lp_time)\n",
        "    print(\"iterations:\", n_iters)\n",
        "    print(\"final envelope size:\", envelope_size)\n",
        "    for name, val in region_flag_final.items():\n",
        "      print(f\"[i-dual] P(ever visit {name}):\", float(val))\n",
        "    for name, val in until_final.items():\n",
        "      print(f\"[i-dual] P({name}):\", float(val))\n",
        "\n",
        "    print(\"P(reach GOAL):\", float(goal_prob_final))\n",
        "    print(\"i-dual objective:\", obj_final)\n",
        "\n",
        "\n",
        "    policy_aug = recover_policy_from_x_aug(mdp_aug, x_final)\n",
        "    base_traj_pctl, aug_traj_pctl = simulate_policy_aug(mdp_aug, policy_aug)\n",
        "    print(\"\\nTrajectory under PCTL-constrained policy (base states):\", base_traj_pctl)\n",
        "\n",
        "    base_policy = collapse_augmented_policy_to_base(mdp_aug, policy_aug)\n",
        "\n",
        "    print(\"\\nFinal policy (collapsed to base MDP):\")\n",
        "    print_policy_grid(mdp, base_policy, G2=G2, G3=G3)\n",
        "\n",
        "elif WORLD == \"20x20_pctl_idual\":\n",
        "    # 1) Base MDP\n",
        "    mdp = make_20x20_world()\n",
        "    # G2 = {(r, c) for r in range(8, 12) for c in range(16, 20)}\n",
        "    # G3 = {(r, c) for r in range(0, 8)  for c in range(0, 8)}\n",
        "\n",
        "    G_corridor = set()\n",
        "\n",
        "    # bottom horizontal segment: (19,0) to (19,5)\n",
        "    for c in range(0, 6):\n",
        "        G_corridor.add((19, c))\n",
        "\n",
        "    # vertical segment: (18,5) up to (11,5)\n",
        "    for r in range(18, 10, -1):   # 18,17,...,11\n",
        "        G_corridor.add((r, 5))\n",
        "\n",
        "    G2 = G_corridor\n",
        "    G3 = {(11, 5)}\n",
        "\n",
        "    flags = [\n",
        "        RegionFlagSpec(\"G2\", G2),\n",
        "        RegionFlagSpec(\"G3\", G3),\n",
        "    ]\n",
        "    until_specs = [\n",
        "        UntilSpec(\"G2U_G3\", A_region=G2, B_region=G3),\n",
        "\n",
        "    ]\n",
        "\n",
        "    mdp_aug = AugmentedMDP(mdp, flags=flags, until_specs=until_specs)\n",
        "    print(f\"Total augmented states (|S_aug|): {len(mdp_aug.states_aug)}\")\n",
        "\n",
        "    p_goal_min = 1 # force eventually reaching the main goal with prob 1\n",
        "\n",
        "    region_constraints = [\n",
        "        # P(ever visit G3) ≤ 0   (hard avoidance)\n",
        "        # PCTLRegionConstraint(\"visit_region_min\", \"G2\", 1),\n",
        "    ]\n",
        "\n",
        "    until_constraints = [\n",
        "        # P >= 0.7 [ G2 U G3 ]\n",
        "        UntilConstraint(\"until_min\", \"G2U_G3\", 1),\n",
        "    ]\n",
        "\n",
        "    constraints_full = region_constraints + until_constraints  # for global LP\n",
        "\n",
        "    p_goal_min = 1\n",
        "    # 1) Unconstrained DP on the base MDP\n",
        "    V_base = value_iteration_shortest_path(mdp)  # returns optimal cost-to-go from each base state\n",
        "\n",
        "    H_cost = {st_aug: V_base[st_aug[0]] for st_aug in mdp_aug.states_aug}\n",
        "\n",
        "    H_region_upper = {\n",
        "    # \"G2\": compute_min_prob_visit_region_before_goal(mdp_aug, \"G2\")\n",
        "\n",
        "    }\n",
        "\n",
        "    H_region_lower = {\n",
        "    # \"G2\": compute_max_prob_visit_region_before_goal(mdp_aug, \"G2\"),\n",
        "    }\n",
        "\n",
        "    H_until_lower = {\n",
        "    # used for until_min (P >= bound)\n",
        "    \"G2U_G3\": compute_max_prob_until_before_goal(mdp_aug, \"G2U_G3\")\n",
        "    }\n",
        "\n",
        "    H_until_upper = {\n",
        "        # \"G2U_G3\": compute_min_prob_until_before_goal(mdp_aug, \"G2U_G3\")\n",
        "    }\n",
        "\n",
        "\n",
        "    (\n",
        "        x_final,\n",
        "        goal_prob_final,\n",
        "        region_flag_final,\n",
        "        until_final,\n",
        "        total_time,\n",
        "        final_lp_time,\n",
        "        n_iters,\n",
        "        envelope_size,\n",
        "        obj_final\n",
        "    ) = i_dual_aug_trevizan(\n",
        "        mdp_aug,\n",
        "        p_goal_min=p_goal_min,\n",
        "        extra_constraints=constraints_full,\n",
        "        H=H_cost,\n",
        "        H_region_upper=H_region_upper,\n",
        "        H_region_lower = H_region_lower,\n",
        "        H_until_upper=H_until_upper,\n",
        "        H_until_lower=H_until_lower,\n",
        "    )\n",
        "\n",
        "    if x_final is None:\n",
        "      print(\"\\n[i-dual] No feasible policy under these constraints.\")\n",
        "    else:\n",
        "      from collections import defaultdict\n",
        "      flow_per_base = defaultdict(float)\n",
        "      for (st, a), val in x_final.items():\n",
        "        if val is None:\n",
        "            continue\n",
        "        (r, c) = st[0]\n",
        "        flow_per_base[(r, c)] += float(val)\n",
        "\n",
        "\n",
        "\n",
        "    print(\"\\n=== timing summary ===\")\n",
        "    print(\"total i-dual time:\", total_time)\n",
        "    print(\"final LP time:\", final_lp_time)\n",
        "    print(\"iterations:\", n_iters)\n",
        "    print(\"final envelope size:\", envelope_size)\n",
        "    print(f\"Total augmented states: {len(mdp_aug.states_aug)}\")\n",
        "    print(f\"Fraction explored:      {envelope_size / len(mdp_aug.states_aug):.4f}\")\n",
        "\n",
        "    for name, val in region_flag_final.items():\n",
        "      print(f\"[i-dual] P(ever visit {name}):\", float(val))\n",
        "    for name, val in until_final.items():\n",
        "      print(f\"[i-dual] P({name}):\", float(val))\n",
        "\n",
        "    print(\"P(reach GOAL):\", float(goal_prob_final))\n",
        "    print(\"i-dual objective:\", obj_final)\n",
        "\n",
        "\n",
        "    policy_aug = recover_policy_from_x_aug(mdp_aug, x_final)\n",
        "    base_traj_pctl, aug_traj_pctl = simulate_policy_aug(mdp_aug, policy_aug)\n",
        "    print(\"\\nTrajectory under PCTL-constrained policy (base states):\", base_traj_pctl)\n",
        "\n",
        "    base_policy = collapse_augmented_policy_to_base(mdp_aug, policy_aug)\n",
        "\n",
        "    print(\"\\nFinal policy (collapsed to base MDP):\")\n",
        "\n",
        "    print_policy_grid(mdp, base_policy, G2=G2, G3=G3)\n",
        "\n",
        "elif WORLD == \"experiment\":\n",
        "    Ns = [5, 20, 30, 40, 50, 70, 100, 150, 200, 250]\n",
        "    # Ns = [5, 7, 10, 15, 20, 25]\n",
        "    # Example custom config:\n",
        "    for N in Ns:\n",
        "      t0 = time.perf_counter()\n",
        "      print(\"\\n==============================\")\n",
        "      print(f\"Experiment for N = {N}\")\n",
        "      print(\"==============================\")\n",
        "      mdp = make_NxN_world(N)\n",
        "\n",
        "      goal = {(0, N-1)}\n",
        "      G2 = {\n",
        "        (r,c)\n",
        "        for r in range(N)\n",
        "        for c in range(N)\n",
        "        if (r, c) not in goal\n",
        "      }\n",
        "      G3 = goal\n",
        "      flags = [\n",
        "        RegionFlagSpec(\"G2\", G2),\n",
        "        RegionFlagSpec(\"G3\", G3),\n",
        "\n",
        "      ]\n",
        "      until_specs = [\n",
        "        UntilSpec(\"G2U_G3\", A_region=G2, B_region=G3),\n",
        "\n",
        "      ]\n",
        "\n",
        "      mdp_aug = AugmentedMDP(mdp, flags=flags, until_specs=until_specs)\n",
        "      # basic size info\n",
        "      S_aug = len(mdp_aug.states_aug)\n",
        "      non_abs = sum(\n",
        "            1 for st in mdp_aug.states_aug\n",
        "            if not mdp_aug.is_absorbing_aug(st)\n",
        "      )\n",
        "      print(f\"|S_base| = {N*N}\")\n",
        "      print(f\"|S_aug|  = {S_aug}\")\n",
        "      print(f\"non-absorbing aug states: {non_abs}\")\n",
        "\n",
        "      p_goal_min = 1\n",
        "\n",
        "      region_constraints = [\n",
        "\n",
        "      ]\n",
        "\n",
        "      until_constraints = [\n",
        "        # P >= 1 [ G2 U G3 ]\n",
        "        UntilConstraint(\"until_min\", \"G2U_G3\", 1),\n",
        "      ]\n",
        "\n",
        "\n",
        "      constraints_full = region_constraints + until_constraints  # for global LP\n",
        "\n",
        "      # 1) Unconstrained DP on the base MDP\n",
        "      V_base = value_iteration_shortest_path(mdp)  # returns optimal cost-to-go from each base state\n",
        "\n",
        "      H_cost = {st_aug: V_base[st_aug[0]] for st_aug in mdp_aug.states_aug}\n",
        "\n",
        "      H_region_upper = {\n",
        "      # \"G2\": compute_min_prob_visit_region_before_goal(mdp_aug, \"G2\")\n",
        "\n",
        "      }\n",
        "\n",
        "      H_region_lower = {\n",
        "      # \"G2\": compute_max_prob_visit_region_before_goal(mdp_aug, \"G2\"),\n",
        "      }\n",
        "\n",
        "      H_until_lower = {\n",
        "      # used for until_min (P >= bound)\n",
        "      \"G2U_G3\": compute_max_prob_until_before_goal(mdp_aug, \"G2U_G3\")\n",
        "      }\n",
        "\n",
        "      H_until_upper = {\n",
        "        # \"G2U_G3\": compute_min_prob_until_before_goal(mdp_aug, \"G2U_G3\")\n",
        "      }\n",
        "\n",
        "      (\n",
        "        x_final,\n",
        "        goal_prob_final,\n",
        "        region_flag_final,\n",
        "        until_final,\n",
        "        total_time,\n",
        "        final_lp_time,\n",
        "        n_iters,\n",
        "        envelope_size,\n",
        "        obj_final\n",
        "      ) = i_dual_aug_trevizan(\n",
        "        mdp_aug,\n",
        "        p_goal_min=p_goal_min,\n",
        "        extra_constraints=constraints_full,\n",
        "        H=H_cost,\n",
        "        H_region_upper=H_region_upper,\n",
        "        H_region_lower = H_region_lower,\n",
        "        H_until_upper=H_until_upper,\n",
        "        H_until_lower=H_until_lower,\n",
        "    )\n",
        "      t1 = time.perf_counter()\n",
        "      time_global = t1 - t0\n",
        "       # 6) Count the number of x-variables\n",
        "      # If x_opt_aug is a dict: {(st,a): value}\n",
        "      if isinstance(x_final, dict):\n",
        "          num_x = len(x_final)\n",
        "      else:\n",
        "          # if it's a numpy array or cvxpy vector:\n",
        "          num_x = x_final.size\n",
        "\n",
        "      print(f\"global LP x-vars (edges):   {num_x}\")\n",
        "      print(f\"optimization time:      {total_time:.4f} s\")\n",
        "      print(f\"full pipeline time:  {time_global:.4f} s\")\n",
        "      print(f\"P(reach GOAL):             {goal_prob_final}\")\n",
        "      print(f\"P(G2U_G3):                 {until_final.get('G2U_G3')}\")\n",
        "      print(f\"Objective value:                 {obj_final:.4f}\")\n",
        "      # How many augmented & base states are actually used by the final policy?\n",
        "      explored_aug_states = {\n",
        "          st for (st, a) in x_final.keys()\n",
        "          if float(x_final[(st, a)]) > 1e-9\n",
        "      }\n",
        "      num_explored_aug = len(explored_aug_states)\n",
        "\n",
        "      explored_base_states = {st_aug[0] for st_aug in explored_aug_states}\n",
        "      num_explored_base = len(explored_base_states)\n",
        "\n",
        "      frac_aug_explored = num_explored_aug / len(mdp_aug.states_aug)\n",
        "      frac_base_explored = num_explored_base / len(mdp.states)\n",
        "\n",
        "      print(f\"explored aug states (flow):  {num_explored_aug} / {len(mdp_aug.states_aug)} \"\n",
        "            f\"({frac_aug_explored:.4%})\")\n",
        "      print(f\"explored base states (flow): {num_explored_base} / {len(mdp.states)} \"\n",
        "            f\"({frac_base_explored:.4%})\")\n",
        "      print(f\"final envelope size (S_hat): {envelope_size} \"\n",
        "            f\"({envelope_size / len(mdp_aug.states_aug):.4%} of S_aug)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97z5W6H-2kzG",
        "outputId": "e1a20b04-1b12-4898-fa8a-ed899fe4e04a"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==============================\n",
            "Experiment for N = 5\n",
            "==============================\n",
            "|S_base| = 25\n",
            "|S_aug|  = 400\n",
            "non-absorbing aug states: 384\n",
            "global LP x-vars (edges):   68\n",
            "optimization time:      0.3769 s\n",
            "full pipeline time:  0.4040 s\n",
            "P(reach GOAL):             1.0000000143904315\n",
            "P(G2U_G3):                 1.0000000143904315\n",
            "Objective value:                 1.7000\n",
            "explored aug states (flow):  16 / 400 (4.0000%)\n",
            "explored base states (flow): 16 / 25 (64.0000%)\n",
            "final envelope size (S_hat): 25 (6.2500% of S_aug)\n",
            "\n",
            "==============================\n",
            "Experiment for N = 20\n",
            "==============================\n",
            "|S_base| = 400\n",
            "|S_aug|  = 6400\n",
            "non-absorbing aug states: 6384\n",
            "global LP x-vars (edges):   188\n",
            "optimization time:      4.2621 s\n",
            "full pipeline time:  5.5454 s\n",
            "P(reach GOAL):             0.9999999998994309\n",
            "P(G2U_G3):                 0.9999999998994309\n",
            "Objective value:                 31.7000\n",
            "explored aug states (flow):  46 / 6400 (0.7188%)\n",
            "explored base states (flow): 46 / 400 (11.5000%)\n",
            "final envelope size (S_hat): 84 (1.3125% of S_aug)\n",
            "\n",
            "==============================\n",
            "Experiment for N = 30\n",
            "==============================\n",
            "|S_base| = 900\n",
            "|S_aug|  = 14400\n",
            "non-absorbing aug states: 14384\n",
            "global LP x-vars (edges):   268\n",
            "optimization time:      9.7624 s\n",
            "full pipeline time:  14.1410 s\n",
            "P(reach GOAL):             1.000000000020723\n",
            "P(G2U_G3):                 1.000000000020723\n",
            "Objective value:                 51.7000\n",
            "explored aug states (flow):  67 / 14400 (0.4653%)\n",
            "explored base states (flow): 67 / 900 (7.4444%)\n",
            "final envelope size (S_hat): 124 (0.8611% of S_aug)\n",
            "\n",
            "==============================\n",
            "Experiment for N = 40\n",
            "==============================\n",
            "|S_base| = 1600\n",
            "|S_aug|  = 25600\n",
            "non-absorbing aug states: 25584\n",
            "global LP x-vars (edges):   348\n",
            "optimization time:      18.2857 s\n",
            "full pipeline time:  28.3475 s\n",
            "P(reach GOAL):             0.9999999989232622\n",
            "P(G2U_G3):                 0.9999999989232622\n",
            "Objective value:                 71.7000\n",
            "explored aug states (flow):  87 / 25600 (0.3398%)\n",
            "explored base states (flow): 87 / 1600 (5.4375%)\n",
            "final envelope size (S_hat): 164 (0.6406% of S_aug)\n",
            "\n",
            "==============================\n",
            "Experiment for N = 50\n",
            "==============================\n",
            "|S_base| = 2500\n",
            "|S_aug|  = 40000\n",
            "non-absorbing aug states: 39984\n",
            "global LP x-vars (edges):   428\n",
            "optimization time:      30.4096 s\n",
            "full pipeline time:  50.2284 s\n",
            "P(reach GOAL):             1.0000000000164941\n",
            "P(G2U_G3):                 1.0000000000164941\n",
            "Objective value:                 91.7000\n",
            "explored aug states (flow):  107 / 40000 (0.2675%)\n",
            "explored base states (flow): 107 / 2500 (4.2800%)\n",
            "final envelope size (S_hat): 204 (0.5100% of S_aug)\n",
            "\n",
            "==============================\n",
            "Experiment for N = 70\n",
            "==============================\n",
            "|S_base| = 4900\n",
            "|S_aug|  = 78400\n",
            "non-absorbing aug states: 78384\n",
            "global LP x-vars (edges):   588\n",
            "optimization time:      65.7902 s\n",
            "full pipeline time:  120.8143 s\n",
            "P(reach GOAL):             0.9999999999314009\n",
            "P(G2U_G3):                 0.9999999999314009\n",
            "Objective value:                 131.7000\n",
            "explored aug states (flow):  146 / 78400 (0.1862%)\n",
            "explored base states (flow): 146 / 4900 (2.9796%)\n",
            "final envelope size (S_hat): 284 (0.3622% of S_aug)\n",
            "\n",
            "==============================\n",
            "Experiment for N = 100\n",
            "==============================\n",
            "|S_base| = 10000\n",
            "|S_aug|  = 160000\n",
            "non-absorbing aug states: 159984\n",
            "global LP x-vars (edges):   828\n",
            "optimization time:      155.0724 s\n",
            "full pipeline time:  318.7719 s\n",
            "P(reach GOAL):             1.000000000025287\n",
            "P(G2U_G3):                 1.000000000025287\n",
            "Objective value:                 191.7000\n",
            "explored aug states (flow):  207 / 160000 (0.1294%)\n",
            "explored base states (flow): 207 / 10000 (2.0700%)\n",
            "final envelope size (S_hat): 404 (0.2525% of S_aug)\n",
            "\n",
            "==============================\n",
            "Experiment for N = 150\n",
            "==============================\n",
            "|S_base| = 22500\n",
            "|S_aug|  = 360000\n",
            "non-absorbing aug states: 359984\n",
            "global LP x-vars (edges):   1228\n",
            "optimization time:      468.0937 s\n",
            "full pipeline time:  1039.8251 s\n",
            "P(reach GOAL):             1.0000000000215938\n",
            "P(G2U_G3):                 1.0000000000215938\n",
            "Objective value:                 291.7000\n",
            "explored aug states (flow):  306 / 360000 (0.0850%)\n",
            "explored base states (flow): 306 / 22500 (1.3600%)\n",
            "final envelope size (S_hat): 604 (0.1678% of S_aug)\n",
            "\n",
            "==============================\n",
            "Experiment for N = 200\n",
            "==============================\n",
            "|S_base| = 40000\n",
            "|S_aug|  = 640000\n",
            "non-absorbing aug states: 639984\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GkoAZNjt_bQi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}